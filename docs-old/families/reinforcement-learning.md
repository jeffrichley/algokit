# Reinforcement Learning Algorithms

## Overview

Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties for its actions and learns to maximize cumulative rewards over time through trial and error.

**Key Characteristics:**
- **Agent-Environment Interaction**: Agent takes actions, environment provides feedback
- **Reward Signal**: Numerical feedback that guides learning
- **Policy**: Strategy that maps states to actions
- **Value Functions**: Estimates of expected future rewards
- **Exploration vs. Exploitation**: Balance between trying new actions and using known good ones

**Common Applications:**
- Game playing (chess, Go, video games)
- Robotics and autonomous systems
- Recommendation systems
- Trading and finance
- Healthcare and drug discovery
- Autonomous vehicles

## Key Concepts

- **Agent**: The learner that interacts with the environment
- **Environment**: The world in which the agent operates
- **State**: Current situation or configuration of the environment
- **Action**: Decision made by the agent
- **Reward**: Numerical feedback from the environment
- **Policy**: Strategy that determines which action to take in each state
- **Value Function**: Expected cumulative reward from a state or state-action pair
- **Q-Function**: Action-value function that estimates expected rewards for state-action pairs

## Comparison Table

| Algorithm | Complexity | Strengths | Weaknesses | Applications |
|-----------|------------|-----------|------------|--------------|
| **Q-Learning** | O(|S|×|A|) time, O(|S|×|A|) space | Model-free, guaranteed convergence, simple | Slow convergence, tabular representation | Discrete environments, simple problems |
| **SARSA** | O(|S|×|A|) time, O(|S|×|A|) space | On-policy, safer exploration, stable | Slower convergence than Q-learning | Online learning, safety-critical applications |
| **DQN** | O(batch_size × network_params) | Handles high-dimensional states, deep representation | Requires experience replay, hyperparameter sensitive | Image-based games, complex environments |
| **Actor-Critic** | O(|S|×|A|) time, O(|S|×|A|) space | Continuous action spaces, stable updates | Two networks to train, potential instability | Continuous control, robotics |
| **Policy Gradient** | O(|S|×|A|) time, O(|S|×|A|) space | Direct policy optimization, natural policy updates | High variance, slow convergence | Policy search, continuous actions |
| **PPO** | O(|S|×|A|) time, O(|S|×|A|) space | Stable training, good sample efficiency | Conservative updates, hyperparameter tuning | Production RL, continuous control |

## Algorithms in This Family

- [**Q-Learning**](../algorithms/reinforcement-learning/q-learning.md) - Model-free value-based learning with temporal difference updates
- [**SARSA**](../algorithms/reinforcement-learning/sarsa.md) - On-policy temporal difference learning for safer exploration
- [**Deep Q-Network (DQN)**](../algorithms/reinforcement-learning/dqn.md) - Deep learning extension with experience replay and target networks
- [**Actor-Critic**](../algorithms/reinforcement-learning/actor-critic.md) - Hybrid approach combining policy gradient and value function methods
- [**Policy Gradient**](../algorithms/reinforcement-learning/policy-gradient.md) - Direct policy optimization with variance reduction techniques
- [**Proximal Policy Optimization (PPO)**](../algorithms/reinforcement-learning/ppo.md) - State-of-the-art stable policy gradient method

## Implementation Status

- **Complete**: 6/6 algorithms (100%)
- **In Progress**: 0/6 algorithms (0%)
- **Planned**: 0/6 algorithms (0%)

## Related Algorithm Families

- **Hierarchical RL**: Extends RL with temporal abstraction and subgoals
- **Multi-Agent RL**: Multiple agents learning in the same environment
- **Inverse RL**: Learning reward functions from expert demonstrations
- **Meta-RL**: Learning to learn new tasks quickly
- **Model-Based RL**: Learning environment dynamics for planning
