# Enhanced Algorithm Schema for Algorithm Documentation
# This schema supports all algorithm types with rich metadata and structured content

# Basic metadata
slug: dqn
name: Deep Q-Network (DQN)
family_id: rl

# Brief one-sentence summary for cards and navigation
hidden: true  # Hidden by default
summary: "A deep reinforcement learning algorithm that uses neural networks to approximate Q-functions for high-dimensional state spaces."

# Detailed description (markdown supported) - full overview for the algorithm page
description: |
  Deep Q-Network (DQN) is a breakthrough algorithm that combines Q-Learning with deep neural networks
  to handle high-dimensional state spaces. It was the first algorithm to successfully learn control
  policies directly from high-dimensional sensory input using end-to-end reinforcement learning.

  DQN addresses the curse of dimensionality by using neural networks to approximate the Q-function,
  making it possible to apply reinforcement learning to complex environments like Atari games,
  robotics, and other real-world applications with continuous or high-dimensional state spaces.

  The algorithm introduces several key innovations including experience replay, target networks,
  and careful preprocessing of inputs to stabilize learning in the deep RL setting.

# Problem formulation and mathematical details
formulation:
  problem_definition: |
    Given:
    - High-dimensional state space: S ⊆ ℝ^n
    - Action space: A
    - Reward function: R(s,a,s')
    - Discount factor: γ ∈ [0,1]
    - Learning rate: α ∈ (0,1]

    Find neural network parameters θ that approximate the optimal Q-function:

    Q*(s,a) ≈ Q(s,a;θ) = NeuralNetwork(s,a;θ)

    Where the network is trained to minimize:
    L(θ) = E[(r + γ max_{a'} Q(s',a';θ^-) - Q(s,a;θ))²]

  key_properties:
    - name: "Function Approximation"
      formula: "Q(s,a) ≈ Q(s,a;θ) = NeuralNetwork(s,a;θ)"
      description: "Uses neural networks to approximate Q-function for high-dimensional states"
    - name: "Experience Replay"
      formula: "Sample random batches from replay buffer D"
      description: "Stores and replays past experiences to break correlation"
    - name: "Target Network"
      formula: "Q(s,a;θ^-) with θ^- updated periodically"
      description: "Uses separate target network to stabilize learning"

# Key properties and characteristics
properties:
  - name: "Deep Learning"
    description: "Uses neural networks for function approximation"
    importance: "fundamental"
  - name: "Experience Replay"
    description: "Stores and replays past experiences to improve sample efficiency"
    importance: "fundamental"
  - name: "Target Network"
    description: "Uses separate target network to stabilize training"
    importance: "fundamental"
  - name: "High-Dimensional Input"
    description: "Can handle raw sensory input like images"
    importance: "fundamental"

# Implementation approaches with detailed code
implementations:
  - type: "basic_dqn"
    name: "Basic DQN"
    description: "Standard DQN with experience replay and target network"
    complexity:
      time: "O(batch_size × network_forward_pass)"
      space: "O(replay_buffer_size + network_parameters)"
    code: |
      import torch
      import torch.nn as nn
      import torch.optim as optim
      import numpy as np
      from collections import deque
      import random

      class DQN(nn.Module):
          """
          Deep Q-Network for high-dimensional state spaces.
          """

          def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
              """
              Initialize DQN network.

              Args:
                  state_dim: Dimension of state space
                  action_dim: Number of actions
                  hidden_dim: Hidden layer dimension
              """
              super(DQN, self).__init__()
              self.fc1 = nn.Linear(state_dim, hidden_dim)
              self.fc2 = nn.Linear(hidden_dim, hidden_dim)
              self.fc3 = nn.Linear(hidden_dim, action_dim)
              self.relu = nn.ReLU()

          def forward(self, x: torch.Tensor) -> torch.Tensor:
              """
              Forward pass through the network.

              Args:
                  x: Input state tensor

              Returns:
                  Q-values for all actions
              """
              x = self.relu(self.fc1(x))
              x = self.relu(self.fc2(x))
              return self.fc3(x)

      class DQNAgent:
          """
          DQN agent with experience replay and target network.
          """

          def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 0.001,
                       gamma: float = 0.99, epsilon: float = 1.0, epsilon_decay: float = 0.995,
                       epsilon_min: float = 0.01, buffer_size: int = 10000,
                       batch_size: int = 32, target_update: int = 100):
              """
              Initialize DQN agent.

              Args:
                  state_dim: Dimension of state space
                  action_dim: Number of actions
                  learning_rate: Learning rate for optimizer
                  gamma: Discount factor
                  epsilon: Initial exploration rate
                  epsilon_decay: Epsilon decay rate
                  epsilon_min: Minimum epsilon value
                  buffer_size: Size of replay buffer
                  batch_size: Batch size for training
                  target_update: Frequency of target network updates
              """
              self.state_dim = state_dim
              self.action_dim = action_dim
              self.gamma = gamma
              self.epsilon = epsilon
              self.epsilon_decay = epsilon_decay
              self.epsilon_min = epsilon_min
              self.batch_size = batch_size
              self.target_update = target_update

              # Initialize networks
              self.q_network = DQN(state_dim, action_dim)
              self.target_network = DQN(state_dim, action_dim)
              self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

              # Initialize replay buffer
              self.replay_buffer = deque(maxlen=buffer_size)
              self.update_count = 0

          def act(self, state: np.ndarray) -> int:
              """
              Choose action using epsilon-greedy policy.

              Args:
                  state: Current state

              Returns:
                  Selected action
              """
              if random.random() < self.epsilon:
                  return random.randint(0, self.action_dim - 1)
              else:
                  state_tensor = torch.FloatTensor(state).unsqueeze(0)
                  q_values = self.q_network(state_tensor)
                  return q_values.argmax().item()

          def remember(self, state: np.ndarray, action: int, reward: float,
                      next_state: np.ndarray, done: bool) -> None:
              """
              Store experience in replay buffer.

              Args:
                  state: Current state
                  action: Action taken
                  reward: Reward received
                  next_state: Next state
                  done: Whether episode is finished
              """
              self.replay_buffer.append((state, action, reward, next_state, done))

          def replay(self) -> None:
              """
              Train the network on a batch of experiences.
              """
              if len(self.replay_buffer) < self.batch_size:
                  return

              # Sample batch from replay buffer
              batch = random.sample(self.replay_buffer, self.batch_size)
              states = torch.FloatTensor([e[0] for e in batch])
              actions = torch.LongTensor([e[1] for e in batch])
              rewards = torch.FloatTensor([e[2] for e in batch])
              next_states = torch.FloatTensor([e[3] for e in batch])
              dones = torch.BoolTensor([e[4] for e in batch])

              # Current Q values
              current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

              # Next Q values from target network
              next_q_values = self.target_network(next_states).max(1)[0].detach()
              target_q_values = rewards + (self.gamma * next_q_values * ~dones)

              # Compute loss
              loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)

              # Optimize
              self.optimizer.zero_grad()
              loss.backward()
              self.optimizer.step()

              # Update target network
              self.update_count += 1
              if self.update_count % self.target_update == 0:
                  self.target_network.load_state_dict(self.q_network.state_dict())

              # Decay epsilon
              if self.epsilon > self.epsilon_min:
                  self.epsilon *= self.epsilon_decay
    advantages:
      - "Handles high-dimensional state spaces"
      - "Can learn from raw sensory input"
      - "Experience replay improves sample efficiency"
      - "Target network stabilizes training"
    disadvantages:
      - "Requires large amounts of data"
      - "Training can be unstable"
      - "Hyperparameter sensitive"
      - "Computationally expensive"

  - type: "double_dqn"
    name: "Double DQN"
    description: "DQN variant that reduces overestimation bias"
    complexity:
      time: "O(batch_size × network_forward_pass)"
      space: "O(replay_buffer_size + network_parameters)"
    code: |
      class DoubleDQNAgent(DQNAgent):
          """
          Double DQN agent that reduces overestimation bias.
          """

          def replay(self) -> None:
              """
              Train the network using Double DQN update rule.
              """
              if len(self.replay_buffer) < self.batch_size:
                  return

              # Sample batch from replay buffer
              batch = random.sample(self.replay_buffer, self.batch_size)
              states = torch.FloatTensor([e[0] for e in batch])
              actions = torch.LongTensor([e[1] for e in batch])
              rewards = torch.FloatTensor([e[2] for e in batch])
              next_states = torch.FloatTensor([e[3] for e in batch])
              dones = torch.BoolTensor([e[4] for e in batch])

              # Current Q values
              current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

              # Double DQN: use main network to select actions, target network to evaluate
              next_actions = self.q_network(next_states).max(1)[1].detach()
              next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
              target_q_values = rewards + (self.gamma * next_q_values * ~dones)

              # Compute loss
              loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)

              # Optimize
              self.optimizer.zero_grad()
              loss.backward()
              self.optimizer.step()

              # Update target network
              self.update_count += 1
              if self.update_count % self.target_update == 0:
                  self.target_network.load_state_dict(self.q_network.state_dict())

              # Decay epsilon
              if self.epsilon > self.epsilon_min:
                  self.epsilon *= self.epsilon_decay
    advantages:
      - "Reduces overestimation bias"
      - "More stable learning"
      - "Better performance in many environments"
      - "Same computational cost as DQN"
    disadvantages:
      - "Still requires large amounts of data"
      - "Hyperparameter sensitive"
      - "Computationally expensive"

# Complexity analysis
complexity:
  analysis:
    - approach: "Basic DQN"
      time: "O(batch_size × network_forward_pass)"
      space: "O(replay_buffer_size + network_parameters)"
      notes: "Time complexity dominated by neural network forward/backward passes. Space complexity includes replay buffer and network parameters"
    
    - approach: "Training Complexity"
      time: "O(episodes × steps_per_episode × batch_size × network_ops)"
      space: "O(replay_buffer_size + 2 × network_parameters)"
      notes: "Training time scales with episodes, steps, and network size. Space includes main and target networks plus replay buffer"

# Applications and use cases
applications:
  - category: "Game Playing"
    examples:
      - "Atari Games: Learning to play classic arcade games"
      - "Chess: Learning chess strategies from board positions"
      - "Go: Learning Go strategies (predecessor to AlphaGo)"
      - "Video Games: Learning to play modern video games"

  - category: "Robotics"
    examples:
      - "Robot Navigation: Learning navigation in complex environments"
      - "Manipulation: Learning to manipulate objects with vision"
      - "Autonomous Vehicles: Learning driving policies from camera input"
      - "Robot Control: Learning control policies for complex tasks"

  - category: "Computer Vision"
    examples:
      - "Image Classification: Learning from visual input"
      - "Object Detection: Learning to detect objects in images"
      - "Scene Understanding: Learning to understand visual scenes"
      - "Visual Navigation: Learning navigation from visual input"

  - category: "Finance"
    examples:
      - "Algorithmic Trading: Learning trading strategies from market data"
      - "Portfolio Management: Learning asset allocation strategies"
      - "Risk Management: Learning risk assessment strategies"
      - "Market Making: Learning market making strategies"

  - category: "Real-World Applications"
    examples:
      - "Recommendation Systems: Learning user preferences from behavior"
      - "Ad Placement: Learning optimal ad positioning"
      - "Energy Management: Learning optimal energy consumption"
      - "Traffic Control: Learning optimal traffic light timing"

  - category: "Educational Value"
    examples:
      - "Deep Learning: Understanding neural networks in RL context"
      - "Function Approximation: Learning to approximate complex functions"
      - "Experience Replay: Understanding importance of data efficiency"
      - "Target Networks: Understanding training stability techniques"

# Educational value and learning objectives
educational_value:
  - "Deep Learning: Understanding neural networks in reinforcement learning"
  - "Function Approximation: Learning to approximate complex value functions"
  - "Experience Replay: Understanding data efficiency in RL"
  - "Target Networks: Understanding training stability techniques"

# Implementation status and development info
status:
  current: "not_started"
  implementation_quality: "none"
  test_coverage: "none"
  documentation_quality: "planned"

  # Source code locations
  source_files:
    - path: "src/algokit/reinforcement_learning/dqn.py"
      description: "Main implementation with basic DQN and variants"
    - path: "tests/unit/reinforcement_learning/test_dqn.py"
      description: "Comprehensive test suite including convergence tests"

# References and resources - structured format for template rendering
references:
  - category: "Core Textbooks"
    items:
      - author: "Sutton, R. S., & Barto, A. G."
        year: "2018"
        title: "Reinforcement Learning: An Introduction"
        publisher: "MIT Press"
        note: "ISBN 978-0-262-03924-6"
      - author: "François-Lavet, V., et al."
        year: "2018"
        title: "An Introduction to Deep Reinforcement Learning"
        publisher: "Foundations and Trends in Machine Learning"
        note: "Volume 11, pages 219-354"

  - category: "DQN"
    items:
      - author: "Mnih, V., et al."
        year: "2015"
        title: "Human-level control through deep reinforcement learning"
        publisher: "Nature"
        note: "Volume 518, pages 529-533"
      - author: "Mnih, V., et al."
        year: "2013"
        title: "Playing Atari with Deep Reinforcement Learning"
        publisher: "arXiv preprint arXiv:1312.5602"
        note: "Original DQN paper"

  - category: "DQN Variants"
    items:
      - author: "Van Hasselt, H., Guez, A., & Silver, D."
        year: "2016"
        title: "Deep Reinforcement Learning with Double Q-learning"
        publisher: "AAAI"
        note: "Double DQN paper"
      - author: "Wang, Z., et al."
        year: "2016"
        title: "Dueling Network Architectures for Deep Reinforcement Learning"
        publisher: "ICML"
        note: "Dueling DQN paper"

  - category: "Online Resources"
    items:
      - title: "Deep Q-Network"
        url: "https://en.wikipedia.org/wiki/Deep_Q-network"
        note: "Wikipedia article on DQN"
      - title: "Deep Reinforcement Learning Tutorial"
        url: "https://spinningup.openai.com/en/latest/"
        note: "OpenAI Spinning Up RL tutorial"
      - title: "DQN Algorithm"
        url: "https://www.geeksforgeeks.org/deep-q-learning/"
        note: "GeeksforGeeks DQN implementation"

  - category: "Implementation & Practice"
    items:
      - title: "Gymnasium (OpenAI Gym)"
        url: "https://gymnasium.farama.org/"
        note: "RL environment library for testing algorithms"
      - title: "Stable Baselines3"
        url: "https://stable-baselines3.readthedocs.io/"
        note: "High-quality RL algorithm implementations"
      - title: "Ray RLlib"
        url: "https://docs.ray.io/en/latest/rllib/"
        note: "Scalable RL library for production use"

# Tags for categorization and search
tags:
  - "rl"
  - "dqn"
  - "deep-learning"
  - "neural-networks"
  - "function-approximation"
  - "algorithms"

# Related algorithms and cross-references
related_algorithms:
  - slug: "q-learning"
    relationship: "same_family"
    description: "Tabular version of Q-Learning that DQN extends"
  - slug: "policy-gradient"
    relationship: "same_family"
    description: "Policy-based alternative to value-based DQN"
