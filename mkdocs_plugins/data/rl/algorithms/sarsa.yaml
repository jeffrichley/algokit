# Enhanced Algorithm Schema for Algorithm Documentation
# This schema supports all algorithm types with rich metadata and structured content

# Basic metadata
slug: sarsa
name: SARSA
family_id: rl

# Brief one-sentence summary for cards and navigation
summary: "An on-policy temporal difference learning algorithm that learns action-value functions by following the current policy."

# Detailed description (markdown supported) - full overview for the algorithm page
description: |
  SARSA (State-Action-Reward-State-Action) is an on-policy temporal difference learning algorithm
  that learns the action-value function Q(s,a) by following the current policy. Unlike Q-Learning,
  SARSA updates its Q-values based on the action actually taken in the next state, making it
  more conservative and safer for online learning scenarios.

  The algorithm is particularly useful when the agent must learn while interacting with the
  environment in real-time, as it considers the exploration strategy in its value updates.
  This makes SARSA more suitable for scenarios where safety is important and the agent
  cannot afford to take potentially dangerous exploratory actions.

# Problem formulation and mathematical details
formulation:
  problem_definition: |
    Given:
    - State space: S
    - Action space: A
    - Reward function: R(s,a,s')
    - Discount factor: γ ∈ [0,1]
    - Learning rate: α ∈ (0,1]
    - Current policy: π

    Find the action-value function Q^π(s,a) for the current policy π:

    Q^π(s,a) = E[∑_{t=0}^∞ γ^t R_{t+1} | S_0 = s, A_0 = a, π]

  key_properties:
    - name: "On-Policy Learning"
      formula: "Q(s,a) ← Q(s,a) + α[r + γ Q(s',a') - Q(s,a)]"
      description: "Updates Q-values based on the action actually taken in the next state"
    - name: "Policy Following"
      formula: "Learns Q^π while following policy π"
      description: "Updates are based on the current policy, not the optimal policy"
    - name: "Conservative Updates"
      formula: "Considers exploration in value updates"
      description: "More conservative than off-policy methods like Q-Learning"

# Key properties and characteristics
properties:
  - name: "On-Policy"
    description: "Learns the value of the policy being followed"
    importance: "fundamental"
  - name: "Model-Free"
    description: "Does not require knowledge of environment dynamics"
    importance: "fundamental"
  - name: "Temporal Difference"
    description: "Updates estimates based on other estimates (bootstrapping)"
    importance: "fundamental"
  - name: "Conservative Learning"
    description: "Considers exploration strategy in value updates"
    importance: "implementation"

# Implementation approaches with detailed code
implementations:
  - type: "tabular_sarsa"
    name: "Tabular SARSA (Classic)"
    description: "Standard SARSA with Q-table for discrete state-action spaces"
    complexity:
      time: "O(|S| × |A| × episodes)"
      space: "O(|S| × |A|)"
    code: |
      import numpy as np
      from typing import Dict, Tuple, Any
      import random

      class SARSA:
          """
          Tabular SARSA implementation for discrete state-action spaces.
          """

          def __init__(self, states: int, actions: int, learning_rate: float = 0.1,
                       discount_factor: float = 0.9, epsilon: float = 0.1):
              """
              Initialize SARSA agent.

              Args:
                  states: Number of states in the environment
                  actions: Number of actions available
                  learning_rate: Learning rate (alpha)
                  discount_factor: Discount factor (gamma)
                  epsilon: Exploration rate for epsilon-greedy policy
              """
              self.states = states
              self.actions = actions
              self.learning_rate = learning_rate
              self.discount_factor = discount_factor
              self.epsilon = epsilon

              # Initialize Q-table with zeros
              self.q_table = np.zeros((states, actions))

          def choose_action(self, state: int) -> int:
              """
              Choose action using epsilon-greedy policy.

              Args:
                  state: Current state

              Returns:
                  Selected action
              """
              if random.random() < self.epsilon:
                  # Explore: choose random action
                  return random.randint(0, self.actions - 1)
              else:
                  # Exploit: choose best action
                  return np.argmax(self.q_table[state])

          def update(self, state: int, action: int, reward: float,
                    next_state: int, next_action: int, done: bool) -> None:
              """
              Update Q-value using SARSA update rule.

              Args:
                  state: Current state
                  action: Action taken
                  reward: Reward received
                  next_state: Next state
                  next_action: Action taken in next state
                  done: Whether episode is finished
              """
              # Current Q-value
              current_q = self.q_table[state, action]

              # Q-value for next state-action pair
              if done:
                  next_q = 0
              else:
                  next_q = self.q_table[next_state, next_action]

              # SARSA update rule
              target = reward + self.discount_factor * next_q
              self.q_table[state, action] = current_q + self.learning_rate * (target - current_q)

          def get_policy(self) -> np.ndarray:
              """
              Get the current policy (greedy with respect to Q-values).

              Returns:
                  Policy array where policy[state] = best_action
              """
              return np.argmax(self.q_table, axis=1)
    advantages:
      - "Safer for online learning scenarios"
      - "Considers exploration in value updates"
      - "More stable learning in some environments"
      - "No model of environment required"
    disadvantages:
      - "May converge to suboptimal policies"
      - "Requires discrete state-action spaces"
      - "Memory requirements grow with state space size"
      - "Slower convergence than off-policy methods"

  - type: "sarsa_lambda"
    name: "SARSA(λ) with Eligibility Traces"
    description: "SARSA with eligibility traces for faster learning"
    complexity:
      time: "O(|S| × |A| × episodes)"
      space: "O(|S| × |A|)"
    code: |
      class SARSALambda:
          """
          SARSA(λ) implementation with eligibility traces.
          """

          def __init__(self, states: int, actions: int, learning_rate: float = 0.1,
                       discount_factor: float = 0.9, epsilon: float = 0.1, 
                       lambda_param: float = 0.9):
              """
              Initialize SARSA(λ) agent.

              Args:
                  states: Number of states in the environment
                  actions: Number of actions available
                  learning_rate: Learning rate (alpha)
                  discount_factor: Discount factor (gamma)
                  epsilon: Exploration rate for epsilon-greedy policy
                  lambda_param: Eligibility trace decay parameter
              """
              self.states = states
              self.actions = actions
              self.learning_rate = learning_rate
              self.discount_factor = discount_factor
              self.epsilon = epsilon
              self.lambda_param = lambda_param

              # Initialize Q-table and eligibility traces
              self.q_table = np.zeros((states, actions))
              self.eligibility_traces = np.zeros((states, actions))

          def update(self, state: int, action: int, reward: float,
                    next_state: int, next_action: int, done: bool) -> None:
              """
              Update Q-values using SARSA(λ) with eligibility traces.

              Args:
                  state: Current state
                  action: Action taken
                  reward: Reward received
                  next_state: Next state
                  next_action: Action taken in next state
                  done: Whether episode is finished
              """
              # Calculate TD error
              current_q = self.q_table[state, action]
              if done:
                  next_q = 0
              else:
                  next_q = self.q_table[next_state, next_action]
              
              td_error = reward + self.discount_factor * next_q - current_q

              # Update eligibility traces
              self.eligibility_traces[state, action] += 1

              # Update all Q-values based on eligibility traces
              self.q_table += self.learning_rate * td_error * self.eligibility_traces
              self.eligibility_traces *= self.discount_factor * self.lambda_param
    advantages:
      - "Faster learning through eligibility traces"
      - "Better sample efficiency"
      - "Maintains on-policy learning benefits"
      - "Can handle delayed rewards more effectively"
    disadvantages:
      - "More complex implementation"
      - "Additional memory for eligibility traces"
      - "Requires tuning of lambda parameter"

# Complexity analysis
complexity:
  analysis:
    - approach: "Tabular SARSA"
      time: "O(|S| × |A| × episodes)"
      space: "O(|S| × |A|)"
      notes: "Time complexity depends on number of episodes and state-action space size. Space required for Q-table grows quadratically with state and action spaces"
    
    - approach: "SARSA(λ)"
      time: "O(|S| × |A| × episodes)"
      space: "O(|S| × |A|)"
      notes: "Similar complexity to tabular SARSA but with eligibility traces for faster convergence"

# Applications and use cases
applications:
  - category: "Online Learning"
    examples:
      - "Real-time Trading: Learning trading strategies while trading"
      - "Robot Navigation: Learning navigation while moving"
      - "Game Playing: Learning strategies while playing"
      - "Resource Management: Learning allocation while managing resources"

  - category: "Safety-Critical Applications"
    examples:
      - "Autonomous Vehicles: Learning driving policies safely"
      - "Medical Systems: Learning treatment protocols"
      - "Industrial Control: Learning control policies for safety"
      - "Financial Systems: Learning investment strategies conservatively"

  - category: "Exploration Scenarios"
    examples:
      - "Scientific Discovery: Learning experimental protocols"
      - "Drug Discovery: Learning molecular design strategies"
      - "Materials Science: Learning synthesis protocols"
      - "Environmental Monitoring: Learning sampling strategies"

  - category: "Educational Value"
    examples:
      - "Reinforcement Learning: Understanding on-policy vs off-policy learning"
      - "Temporal Difference Learning: Understanding policy-dependent updates"
      - "Exploration vs Exploitation: Understanding conservative learning"
      - "Value Functions: Understanding policy evaluation"

# Educational value and learning objectives
educational_value:
  - "On-Policy Learning: Understanding the difference between on-policy and off-policy methods"
  - "Conservative Learning: Learning about safer learning approaches"
  - "Policy Evaluation: Understanding how to evaluate current policies"
  - "Temporal Difference Learning: Understanding policy-dependent value updates"

# Implementation status and development info
status:
  current: "not_started"
  implementation_quality: "none"
  test_coverage: "none"
  documentation_quality: "planned"

  # Source code locations
  source_files:
    - path: "src/algokit/reinforcement_learning/sarsa.py"
      description: "Main implementation with tabular and function approximation variants"
    - path: "tests/unit/reinforcement_learning/test_sarsa.py"
      description: "Comprehensive test suite including convergence tests"

# References and resources - structured format for template rendering
references:
  - category: "Core Textbooks"
    items:
      - author: "Sutton, R. S., & Barto, A. G."
        year: "2018"
        title: "Reinforcement Learning: An Introduction"
        publisher: "MIT Press"
        note: "ISBN 978-0-262-03924-6"
      - author: "Szepesvári, C."
        year: "2010"
        title: "Algorithms for Reinforcement Learning"
        publisher: "Morgan & Claypool"
        note: "ISBN 978-1-60845-492-1"

  - category: "SARSA"
    items:
      - author: "Rummery, G. A., & Niranjan, M."
        year: "1994"
        title: "On-line Q-learning using connectionist systems"
        publisher: "Cambridge University Engineering Department"
        note: "Technical Report CUED/F-INFENG/TR 166"
      - author: "Sutton, R. S."
        year: "1996"
        title: "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"
        publisher: "Advances in Neural Information Processing Systems"
        note: "Volume 8, pages 1038-1044"

  - category: "Online Resources"
    items:
      - title: "SARSA"
        url: "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action"
        note: "Wikipedia article on SARSA"
      - title: "Reinforcement Learning Tutorial"
        url: "https://spinningup.openai.com/en/latest/"
        note: "OpenAI Spinning Up RL tutorial"
      - title: "SARSA Algorithm"
        url: "https://www.geeksforgeeks.org/sarsa-reinforcement-learning/"
        note: "GeeksforGeeks SARSA implementation"

  - category: "Implementation & Practice"
    items:
      - title: "Gymnasium (OpenAI Gym)"
        url: "https://gymnasium.farama.org/"
        note: "RL environment library for testing algorithms"
      - title: "Stable Baselines3"
        url: "https://stable-baselines3.readthedocs.io/"
        note: "High-quality RL algorithm implementations"
      - title: "Ray RLlib"
        url: "https://docs.ray.io/en/latest/rllib/"
        note: "Scalable RL library for production use"

# Tags for categorization and search
tags:
  - "rl"
  - "sarsa"
  - "temporal-difference"
  - "model-free"
  - "on-policy"
  - "algorithms"

# Related algorithms and cross-references
related_algorithms:
  - slug: "q-learning"
    relationship: "same_family"
    description: "Off-policy alternative to SARSA with similar update rule"
  - slug: "policy-gradient"
    relationship: "same_family"
    description: "Policy-based alternative to value-based SARSA"
