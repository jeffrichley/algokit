# Enhanced Algorithm Schema for Algorithm Documentation
# This schema supports all algorithm types with rich metadata and structured content

# Basic metadata
slug: dmps-manipulation
name: DMPs for Manipulation
family_id: dmps

# Brief one-sentence summary for cards and navigation
hidden: true  # Hidden by default
summary: "DMPs specialized for robotic manipulation tasks including grasping movements, assembly tasks, and tool use behaviors."

# Detailed description (markdown supported) - full overview for the algorithm page
description: |
  DMPs for Manipulation extend the basic DMP framework to handle robotic manipulation tasks such as grasping, assembly, and tool use. This approach enables robots to learn and execute complex manipulation behaviors with proper force control, object interaction, and task-specific adaptations.

  The key innovation of manipulation DMPs is the integration of:
  - Grasping movement generation with proper approach and grasp strategies
  - Assembly task coordination with precise positioning and force control
  - Tool use behaviors with appropriate tool manipulation techniques
  - Object interaction modeling with force and contact considerations
  - Task-specific adaptations for different manipulation scenarios

  These DMPs are particularly valuable in applications requiring precise manipulation, such as industrial assembly, household tasks, and any scenario where robots must interact with objects in their environment.

# Problem formulation and mathematical details
formulation:
  problem_definition: |
    Given:
    - End-effector position: p(t) = [x(t), y(t), z(t)]
    - End-effector orientation: R(t) ∈ SO(3)
    - Grasp configuration: g(t) = [g_1(t), g_2(t), ..., g_n(t)]
    - Object properties: O = {position, orientation, size, weight, friction}
    - Task constraints: C = {workspace_limits, force_limits, collision_avoidance}

    The manipulation DMP becomes:
    τp̈ = α_y(β_y(p_goal - p) - ṗ) + f_p(x) + f_force(p, O) + f_constraint(p, C)
    τR̈ = α_y(β_y(R_goal - R) - Ṙ) + f_R(x) + f_orientation(R, O)
    τg̈ = α_y(β_y(g_goal - g) - ġ) + f_g(x) + f_grasp(g, O)
    
    Where:
    - f_p, f_R, f_g are the forcing functions for position, orientation, and grasp
    - f_force models object interaction forces
    - f_constraint enforces task constraints
    - f_orientation handles orientation control
    - f_grasp manages grasp configuration

  key_properties:
    - name: "Position Control"
      formula: "τp̈ = α_y(β_y(p_goal - p) - ṗ) + f_p(x) + f_force(p, O)"
      description: "Controls end-effector position with force feedback"
    - name: "Orientation Control"
      formula: "τR̈ = α_y(β_y(R_goal - R) - Ṙ) + f_R(x) + f_orientation(R, O)"
      description: "Controls end-effector orientation"
    - name: "Grasp Control"
      formula: "τg̈ = α_y(β_y(g_goal - g) - ġ) + f_g(x) + f_grasp(g, O)"
      description: "Controls grasp configuration"

# Key properties and characteristics
properties:
  - name: "Grasping Movements"
    description: "Generates appropriate grasping movements with proper approach strategies"
    importance: "fundamental"
  - name: "Assembly Coordination"
    description: "Coordinates complex assembly tasks with precise positioning"
    importance: "fundamental"
  - name: "Tool Use Behaviors"
    description: "Enables tool use with appropriate manipulation techniques"
    importance: "fundamental"
  - name: "Object Interaction"
    description: "Models object interaction with force and contact considerations"
    importance: "fundamental"

# Implementation approaches with detailed code
implementations:
  - type: "grasping_dmp"
    name: "Grasping DMPs"
    description: "DMPs for generating grasping movements with proper approach and grasp strategies"
    complexity:
      time: "O(T × K × G)"
      space: "O(K × G)"
    code: |
      import numpy as np
      from scipy.integrate import odeint
      from typing import Dict, List, Tuple, Optional
      import matplotlib.pyplot as plt

      class GraspingDMP:
          """
          DMP for generating grasping movements with proper approach and grasp strategies.
          """

          def __init__(self, n_dims: int = 6, n_basis: int = 50, alpha_y: float = 25.0, 
                       beta_y: float = 6.25, alpha_x: float = 1.0, n_fingers: int = 5):
              """
              Initialize grasping DMP.

              Args:
                  n_dims: Number of dimensions (3 for position + 3 for orientation)
                  n_basis: Number of basis functions
                  alpha_y: Spring constant for transformation system
                  beta_y: Damping constant for transformation system
                  alpha_x: Decay rate for canonical system
                  n_fingers: Number of fingers for grasping
              """
              self.n_dims = n_dims
              self.n_basis = n_basis
              self.alpha_y = alpha_y
              self.beta_y = beta_y
              self.alpha_x = alpha_x
              self.n_fingers = n_fingers
              
              # Basis function parameters
              self.c = np.exp(-alpha_x * np.linspace(0, 1, n_basis))
              self.h = np.ones(n_basis) * n_basis / np.sum(self.c)
              
              # DMP weights for end-effector
              self.w_position = np.zeros((3, n_basis))
              self.w_orientation = np.zeros((3, n_basis))
              
              # DMP weights for fingers
              self.w_fingers = np.zeros((n_fingers, n_basis))
              
              # Grasping parameters
              self.approach_distance = 0.1
              self.grasp_force = 10.0
              self.grasp_width = 0.05
              
              # Object information
              self.object_position = np.zeros(3)
              self.object_orientation = np.eye(3)
              self.object_size = np.array([0.05, 0.05, 0.05])

          def set_object_properties(self, position: np.ndarray, orientation: np.ndarray, 
                                  size: np.ndarray) -> None:
              """
              Set object properties for grasping.

              Args:
                  position: Object position
                  orientation: Object orientation
                  size: Object size
              """
              self.object_position = position.copy()
              self.object_orientation = orientation.copy()
              self.object_size = size.copy()

          def set_grasp_parameters(self, approach_distance: float, grasp_force: float, 
                                 grasp_width: float) -> None:
              """
              Set grasping parameters.

              Args:
                  approach_distance: Distance for approach phase
                  grasp_force: Force for grasping
                  grasp_width: Width of grasp
              """
              self.approach_distance = approach_distance
              self.grasp_force = grasp_force
              self.grasp_width = grasp_width

          def compute_approach_trajectory(self, start_position: np.ndarray, 
                                        start_orientation: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
              """
              Compute approach trajectory for grasping.

              Args:
                  start_position: Start position of end-effector
                  start_orientation: Start orientation of end-effector

              Returns:
                  Tuple of (approach_position, approach_orientation)
              """
              # Compute approach position (offset from object)
              approach_direction = self.object_position - start_position
              approach_direction = approach_direction / (np.linalg.norm(approach_direction) + 1e-10)
              
              approach_position = self.object_position - approach_direction * self.approach_distance
              
              # Compute approach orientation (aligned with object)
              approach_orientation = self.object_orientation.copy()
              
              return approach_position, approach_orientation

          def compute_grasp_trajectory(self, approach_position: np.ndarray, 
                                     approach_orientation: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
              """
              Compute grasp trajectory.

              Args:
                  approach_position: Approach position
                  approach_orientation: Approach orientation

              Returns:
                  Tuple of (grasp_position, grasp_orientation, finger_positions)
              """
              # Grasp position (at object center)
              grasp_position = self.object_position.copy()
              
              # Grasp orientation (aligned with object)
              grasp_orientation = self.object_orientation.copy()
              
              # Finger positions (around object)
              finger_positions = np.zeros((self.n_fingers, 3))
              for i in range(self.n_fingers):
                  angle = 2 * np.pi * i / self.n_fingers
                  finger_positions[i] = grasp_position + np.array([
                      self.grasp_width * np.cos(angle),
                      self.grasp_width * np.sin(angle),
                      0.0
                  ])
              
              return grasp_position, grasp_orientation, finger_positions

          def learn_from_demo(self, position_demo: np.ndarray, orientation_demo: np.ndarray,
                            finger_demo: np.ndarray, dt: float) -> None:
              """
              Learn grasping DMP from demonstration.

              Args:
                  position_demo: Demonstrated position trajectory [T, 3]
                  orientation_demo: Demonstrated orientation trajectory [T, 3, 3]
                  finger_demo: Demonstrated finger trajectory [T, n_fingers]
                  dt: Time step
              """
              T = len(position_demo)
              
              # Generate canonical system trajectory
              x = np.exp(-self.alpha_x * np.linspace(0, 1, T))
              
              # Learn position weights
              for d in range(3):
                  f_target = np.gradient(position_demo[:, d], dt)
                  f_target = np.gradient(f_target, dt)
                  
                  for i in range(self.n_basis):
                      psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                      numerator = np.sum(psi * x * f_target)
                      denominator = np.sum(psi * x**2)
                      
                      if denominator > 1e-10:
                          self.w_position[d, i] = numerator / denominator
              
              # Learn orientation weights (simplified)
              for d in range(3):
                  f_target = np.gradient(orientation_demo[:, d, d], dt)
                  f_target = np.gradient(f_target, dt)
                  
                  for i in range(self.n_basis):
                      psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                      numerator = np.sum(psi * x * f_target)
                      denominator = np.sum(psi * x**2)
                      
                      if denominator > 1e-10:
                          self.w_orientation[d, i] = numerator / denominator
              
              # Learn finger weights
              for finger_id in range(self.n_fingers):
                  f_target = np.gradient(finger_demo[:, finger_id], dt)
                  f_target = np.gradient(f_target, dt)
                  
                  for i in range(self.n_basis):
                      psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                      numerator = np.sum(psi * x * f_target)
                      denominator = np.sum(psi * x**2)
                      
                      if denominator > 1e-10:
                          self.w_fingers[finger_id, i] = numerator / denominator

          def generate_grasping_trajectory(self, start_position: np.ndarray, start_orientation: np.ndarray,
                                         tau: float = 1.0, dt: float = 0.01) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
              """
              Generate grasping trajectory.

              Args:
                  start_position: Start position of end-effector
                  start_orientation: Start orientation of end-effector
                  tau: Temporal scaling factor
                  dt: Time step

              Returns:
                  Tuple of (position_trajectory, orientation_trajectory, finger_trajectory)
              """
              # Compute approach and grasp targets
              approach_pos, approach_orient = self.compute_approach_trajectory(start_position, start_orientation)
              grasp_pos, grasp_orient, finger_positions = self.compute_grasp_trajectory(approach_pos, approach_orient)
              
              # Integration time
              t_span = np.arange(0, 1.0, dt / tau)
              T = len(t_span)
              
              # Storage
              position_traj = np.zeros((T, 3))
              orientation_traj = np.zeros((T, 3, 3))
              finger_traj = np.zeros((T, self.n_fingers))
              
              # Initial conditions
              position_traj[0] = start_position
              orientation_traj[0] = start_orientation
              finger_traj[0] = np.zeros(self.n_fingers)
              
              # Generate trajectory
              for t in range(1, T):
                  x = np.exp(-self.alpha_x * t_span[t])
                  
                  # Update position
                  for d in range(3):
                      f = 0.0
                      for i in range(self.n_basis):
                          psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                          f += self.w_position[d, i] * psi * x
                      
                      # DMP dynamics
                      ddy = self.alpha_y * (self.beta_y * (grasp_pos[d] - position_traj[t-1, d]) - 0) + f
                      position_traj[t, d] = position_traj[t-1, d] + ddy * dt**2
                  
                  # Update orientation (simplified)
                  for d in range(3):
                      f = 0.0
                      for i in range(self.n_basis):
                          psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                          f += self.w_orientation[d, i] * psi * x
                      
                      # DMP dynamics
                      ddy = self.alpha_y * (self.beta_y * (grasp_orient[d, d] - orientation_traj[t-1, d, d]) - 0) + f
                      orientation_traj[t, d, d] = orientation_traj[t-1, d, d] + ddy * dt**2
                  
                  # Update fingers
                  for finger_id in range(self.n_fingers):
                      f = 0.0
                      for i in range(self.n_basis):
                          psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                          f += self.w_fingers[finger_id, i] * psi * x
                      
                      # DMP dynamics
                      ddy = self.alpha_y * (self.beta_y * (self.grasp_force - finger_traj[t-1, finger_id]) - 0) + f
                      finger_traj[t, finger_id] = finger_traj[t-1, finger_id] + ddy * dt**2
              
              return position_traj, orientation_traj, finger_traj

          def visualize_grasping_trajectory(self, position_traj: np.ndarray, 
                                          orientation_traj: np.ndarray, 
                                          finger_traj: np.ndarray) -> None:
              """
              Visualize the grasping trajectory.

              Args:
                  position_traj: Position trajectory
                  orientation_traj: Orientation trajectory
                  finger_traj: Finger trajectory
              """
              fig = plt.figure(figsize=(15, 10))
              
              # 3D trajectory plot
              ax1 = fig.add_subplot(221, projection='3d')
              ax1.plot(position_traj[:, 0], position_traj[:, 1], position_traj[:, 2], 'b-', linewidth=2, label='End-effector')
              ax1.scatter(self.object_position[0], self.object_position[1], self.object_position[2], 'ro', s=100, label='Object')
              ax1.set_xlabel('X Position')
              ax1.set_ylabel('Y Position')
              ax1.set_zlabel('Z Position')
              ax1.set_title('3D Grasping Trajectory')
              ax1.legend()
              
              # Position components
              ax2 = fig.add_subplot(222)
              t = np.arange(len(position_traj))
              ax2.plot(t, position_traj[:, 0], 'r-', label='X')
              ax2.plot(t, position_traj[:, 1], 'g-', label='Y')
              ax2.plot(t, position_traj[:, 2], 'b-', label='Z')
              ax2.set_xlabel('Time')
              ax2.set_ylabel('Position')
              ax2.set_title('Position Components')
              ax2.legend()
              ax2.grid(True)
              
              # Finger forces
              ax3 = fig.add_subplot(223)
              for finger_id in range(self.n_fingers):
                  ax3.plot(t, finger_traj[:, finger_id], label=f'Finger {finger_id+1}')
              ax3.set_xlabel('Time')
              ax3.set_ylabel('Force')
              ax3.set_title('Finger Forces')
              ax3.legend()
              ax3.grid(True)
              
              # Orientation
              ax4 = fig.add_subplot(224)
              for d in range(3):
                  ax4.plot(t, orientation_traj[:, d, d], label=f'Orientation {d+1}')
              ax4.set_xlabel('Time')
              ax4.set_ylabel('Orientation')
              ax4.set_title('Orientation Components')
              ax4.legend()
              ax4.grid(True)
              
              plt.tight_layout()
              plt.show()

    advantages:
      - "Natural grasping movements"
      - "Proper approach strategies"
      - "Force control integration"
      - "Multi-finger coordination"
    disadvantages:
      - "Requires object information"
      - "Complex finger coordination"
      - "May not handle all object types"

  - type: "assembly_dmp"
    name: "Assembly DMPs"
    description: "DMPs for assembly tasks with precise positioning and force control"
    complexity:
      time: "O(T × K × A)"
      space: "O(K × A)"
    code: |
      class AssemblyDMP(GraspingDMP):
          """
          DMP for assembly tasks with precise positioning and force control.
          """

          def __init__(self, n_dims: int = 6, n_basis: int = 50, alpha_y: float = 25.0, 
                       beta_y: float = 6.25, alpha_x: float = 1.0, n_fingers: int = 5,
                       assembly_tolerance: float = 0.001):
              """
              Initialize assembly DMP.

              Args:
                  n_dims: Number of dimensions
                  n_basis: Number of basis functions
                  alpha_y: Spring constant for transformation system
                  beta_y: Damping constant for transformation system
                  alpha_x: Decay rate for canonical system
                  n_fingers: Number of fingers
                  assembly_tolerance: Tolerance for assembly positioning
              """
              super().__init__(n_dims, n_basis, alpha_y, beta_y, alpha_x, n_fingers)
              self.assembly_tolerance = assembly_tolerance
              
              # Assembly parameters
              self.assembly_force = 5.0
              self.insertion_depth = 0.02
              self.assembly_speed = 0.01
              
              # Assembly sequence
              self.assembly_sequence = []
              self.current_step = 0

          def set_assembly_sequence(self, sequence: List[Dict]) -> None:
              """
              Set assembly sequence.

              Args:
                  sequence: List of assembly steps
              """
              self.assembly_sequence = sequence
              self.current_step = 0

          def set_assembly_parameters(self, assembly_force: float, insertion_depth: float, 
                                    assembly_speed: float) -> None:
              """
              Set assembly parameters.

              Args:
                  assembly_force: Force for assembly
                  insertion_depth: Depth of insertion
                  assembly_speed: Speed of assembly
              """
              self.assembly_force = assembly_force
              self.insertion_depth = insertion_depth
              self.assembly_speed = assembly_speed

          def compute_assembly_trajectory(self, start_position: np.ndarray, 
                                        start_orientation: np.ndarray,
                                        target_position: np.ndarray,
                                        target_orientation: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
              """
              Compute assembly trajectory.

              Args:
                  start_position: Start position
                  start_orientation: Start orientation
                  target_position: Target position
                  target_orientation: Target orientation

              Returns:
                  Tuple of (assembly_position, assembly_orientation)
              """
              # Assembly position (with insertion depth)
              assembly_position = target_position + np.array([0, 0, self.insertion_depth])
              
              # Assembly orientation (aligned with target)
              assembly_orientation = target_orientation.copy()
              
              return assembly_position, assembly_orientation

          def generate_assembly_trajectory(self, start_position: np.ndarray, start_orientation: np.ndarray,
                                         target_position: np.ndarray, target_orientation: np.ndarray,
                                         tau: float = 1.0, dt: float = 0.01) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
              """
              Generate assembly trajectory.

              Args:
                  start_position: Start position
                  start_orientation: Start orientation
                  target_position: Target position
                  target_orientation: Target orientation
                  tau: Temporal scaling factor
                  dt: Time step

              Returns:
                  Tuple of (position_trajectory, orientation_trajectory, force_trajectory)
              """
              # Compute assembly targets
              assembly_pos, assembly_orient = self.compute_assembly_trajectory(
                  start_position, start_orientation, target_position, target_orientation)
              
              # Integration time
              t_span = np.arange(0, 1.0, dt / tau)
              T = len(t_span)
              
              # Storage
              position_traj = np.zeros((T, 3))
              orientation_traj = np.zeros((T, 3, 3))
              force_traj = np.zeros((T, 3))
              
              # Initial conditions
              position_traj[0] = start_position
              orientation_traj[0] = start_orientation
              force_traj[0] = np.zeros(3)
              
              # Generate trajectory
              for t in range(1, T):
                  x = np.exp(-self.alpha_x * t_span[t])
                  
                  # Update position
                  for d in range(3):
                      f = 0.0
                      for i in range(self.n_basis):
                          psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                          f += self.w_position[d, i] * psi * x
                      
                      # DMP dynamics
                      ddy = self.alpha_y * (self.beta_y * (assembly_pos[d] - position_traj[t-1, d]) - 0) + f
                      position_traj[t, d] = position_traj[t-1, d] + ddy * dt**2
                  
                  # Update orientation
                  for d in range(3):
                      f = 0.0
                      for i in range(self.n_basis):
                          psi = np.exp(-self.h[i] * (x - self.c[i])**2)
                          f += self.w_orientation[d, i] * psi * x
                      
                      # DMP dynamics
                      ddy = self.alpha_y * (self.beta_y * (assembly_orient[d, d] - orientation_traj[t-1, d, d]) - 0) + f
                      orientation_traj[t, d, d] = orientation_traj[t-1, d, d] + ddy * dt**2
                  
                  # Update force
                  for d in range(3):
                      if d == 2:  # Z-direction (insertion)
                          force_traj[t, d] = self.assembly_force
                      else:
                          force_traj[t, d] = 0.0
              
              return position_traj, orientation_traj, force_traj

    advantages:
      - "Precise assembly positioning"
      - "Force control integration"
      - "Assembly sequence coordination"
      - "Tolerance handling"
    disadvantages:
      - "Requires precise positioning"
      - "Complex force control"
      - "May not handle all assembly types"

# Complexity analysis
complexity:
  analysis:
    - approach: "Grasping DMP"
      time: "O(T × K × G)"
      space: "O(K × G)"
      notes: "Time complexity scales with trajectory length, basis functions, and grasp complexity"
    
    - approach: "Assembly DMP"
      time: "O(T × K × A)"
      space: "O(K × A)"
      notes: "Additional complexity for assembly coordination"
    
    - approach: "Object Interaction"
      time: "O(T × O)"
      space: "O(O)"
      notes: "Object interaction scales with trajectory length and object complexity"

# Applications and use cases
applications:
  - category: "Industrial Assembly"
    examples:
      - "Product Assembly: Assembling products with multiple components"
      - "Quality Control: Quality control tasks with precise positioning"
      - "Packaging: Packaging tasks with object manipulation"
      - "Inspection: Inspection tasks with object handling"

  - category: "Household Tasks"
    examples:
      - "Cooking: Cooking tasks with utensil manipulation"
      - "Cleaning: Cleaning tasks with tool use"
      - "Laundry: Laundry tasks with object handling"
      - "Gardening: Gardening tasks with tool manipulation"

  - category: "Service Robotics"
    examples:
      - "Healthcare: Healthcare tasks with medical tool use"
      - "Education: Educational tasks with object manipulation"
      - "Entertainment: Entertainment tasks with object interaction"
      - "Security: Security tasks with object handling"

  - category: "Human-Robot Interaction"
    examples:
      - "Collaborative Tasks: Collaborative tasks with object sharing"
      - "Assistive Tasks: Assistive tasks with object manipulation"
      - "Social Interaction: Social interaction tasks with object handling"
      - "Learning Tasks: Learning tasks with object demonstration"

  - category: "Research Applications"
    examples:
      - "Manipulation Research: Studying manipulation principles"
      - "Grasping Research: Studying grasping strategies"
      - "Assembly Research: Studying assembly techniques"
      - "Tool Use Research: Studying tool use behaviors"

# Educational value and learning objectives
educational_value:
  - "Manipulation: Understanding manipulation principles and techniques"
  - "Grasping: Understanding grasping strategies and force control"
  - "Assembly: Understanding assembly coordination and precision"
  - "Tool Use: Understanding tool use behaviors and techniques"

# Implementation status and development info
status:
  current: "not_started"
  implementation_quality: "none"
  test_coverage: "none"
  documentation_quality: "planned"

  # Source code locations
  source_files:
    - path: "src/algokit/dynamic_movement_primitives/manipulation_dmps.py"
      description: "Main implementation with grasping and assembly DMPs"
    - path: "tests/unit/dynamic_movement_primitives/test_manipulation_dmps.py"
      description: "Comprehensive test suite including manipulation tests"

# References and resources - structured format for template rendering
references:
  - category: "Core Papers"
    items:
      - author: "Ijspeert, A. J., Nakanishi, J., Hoffmann, H., Pastor, P., & Schaal, S."
        year: "2013"
        title: "Dynamical movement primitives: Learning attractor landscapes for motor skills"
        publisher: "Biological Cybernetics"
        note: "Comprehensive review of DMPs including manipulation applications"
      - author: "Gams, A., Nemec, B., Ijspeert, A. J., & Ude, A."
        year: "2014"
        title: "Coupling movement primitives: Interaction with the environment and bimanual tasks"
        publisher: "IEEE Transactions on Robotics"
        note: "DMPs for manipulation and bimanual tasks"

  - category: "Manipulation"
    items:
      - author: "Mason, M. T."
        year: "2001"
        title: "Mechanics of robotic manipulation"
        publisher: "MIT Press"
        note: "Fundamental work on robotic manipulation"
      - author: "Murray, R. M., Li, Z., & Sastry, S. S."
        year: "1994"
        title: "A mathematical introduction to robotic manipulation"
        publisher: "CRC Press"
        note: "Mathematical foundations of robotic manipulation"

  - category: "Online Resources"
    items:
      - title: "Robotic Manipulation"
        url: "https://en.wikipedia.org/wiki/Robotic_manipulation"
        note: "Wikipedia article on robotic manipulation"
      - title: "Grasping"
        url: "https://en.wikipedia.org/wiki/Grasping"
        note: "Wikipedia article on grasping"
      - title: "Assembly Line"
        url: "https://en.wikipedia.org/wiki/Assembly_line"
        note: "Wikipedia article on assembly lines"

  - category: "Implementation & Practice"
    items:
      - title: "ROS MoveIt"
        url: "https://moveit.ros.org/"
        note: "ROS motion planning framework for manipulation"
      - title: "ROS Control"
        url: "https://ros.org/reps/rep-0144.html"
        note: "ROS control framework for robot control"
      - title: "ROS Perception"
        url: "https://ros.org/reps/rep-0118.html"
        note: "ROS perception stack for object detection"

# Tags for categorization and search
tags:
  - "dmps"
  - "manipulation"
  - "grasping"
  - "assembly"
  - "tool-use"
  - "object-interaction"

# Related algorithms and cross-references
related_algorithms:
  - slug: "basic-dmps"
    relationship: "same_family"
    description: "Basic DMPs that manipulation DMPs extend for object interaction"
  - slug: "spatially-coupled-bimanual-dmps"
    relationship: "same_family"
    description: "Bimanual DMPs that can be combined with manipulation for coordinated tasks"
