# Enhanced Algorithm Schema for Algorithm Documentation
# This schema supports all algorithm types with rich metadata and structured content

# Basic metadata
slug: hierarchical-task-networks
name: Hierarchical Task Networks (HTNs)
family_id: hierarchical-rl

# Brief one-sentence summary for cards and navigation
hidden: true  # Hidden by default
summary: "A hierarchical reinforcement learning approach that decomposes complex tasks into hierarchical structures of subtasks for planning and execution."

# Detailed description (markdown supported) - full overview for the algorithm page
description: |
  Hierarchical Task Networks (HTNs) represent a powerful approach to reinforcement learning that
  decomposes complex tasks into hierarchical structures of subtasks. The algorithm learns to plan
  and execute tasks at multiple levels of abstraction, where high-level tasks are broken down into
  simpler subtasks that can be learned and executed independently.

  This hierarchical approach enables agents to solve complex, long-horizon problems by leveraging
  task decomposition and temporal abstraction. HTNs are particularly effective in domains where
  tasks have natural hierarchical structure, such as robotics manipulation, autonomous navigation,
  and complex game playing scenarios.

# Problem formulation and mathematical details
formulation:
  problem_definition: |
    Given:
    - Task hierarchy: T = {T_1, T_2, ..., T_n}
    - Subtask decomposition: T_i = {t_{i1}, t_{i2}, ..., t_{im}}
    - State space: S
    - Action space: A
    - Reward function: R(s,a,s')

    Find hierarchical task policies that maximize expected cumulative reward:

    V(T_i) = max_π E_{τ ~ π}[∑_{t=0}^H γ^t r_t]

    Where π is the policy for executing the task decomposition, and H is the horizon.

  key_properties:
    - name: "Task Decomposition"
      formula: "T_i = {t_{i1}, t_{i2}, ..., t_{im}}"
      description: "Complex tasks broken into simpler subtasks"
    - name: "Hierarchical Value Function"
      formula: "V(T_i) = max_π E_{τ ~ π}[∑_{t=0}^H γ^t r_t]"
      description: "Value function for composite tasks"
    - name: "Subtask Policy"
      formula: "π_{t_{ij}}(a|s) = argmax_a Q_{t_{ij}}(s, a)"
      description: "Policy for executing individual subtasks"

# Key properties and characteristics
properties:
  - name: "Task Decomposition"
    description: "Complex tasks broken into manageable subtasks"
    importance: "fundamental"
  - name: "Temporal Abstraction"
    description: "Different levels operate at different time scales"
    importance: "fundamental"
  - name: "Modular Learning"
    description: "Subtasks can be learned independently"
    importance: "fundamental"
  - name: "Reusability"
    description: "Learned subtasks can be applied to different composite tasks"
    importance: "implementation"

# Implementation approaches with detailed code
implementations:
  - type: "basic_hierarchical_task_networks"
    name: "Basic Hierarchical Task Networks (Recommended)"
    description: "Standard HTN implementation with task hierarchy and subtask policies"
    complexity:
      time: "O(|T| × |S| × |A| × episodes)"
      space: "O(|T| × |S| × |A|)"
    code: |
      import numpy as np
      from typing import Dict, List, Tuple, Any
      from dataclasses import dataclass

      @dataclass
      class Task:
          """Represents a task in the hierarchy."""
          name: str
          subtasks: List['Task']
          policy: Any = None
          value: float = 0.0

          def is_primitive(self) -> bool:
              """Check if this is a primitive task (no subtasks)."""
              return len(self.subtasks) == 0

      class HierarchicalTaskNetworkAgent:
          """
          Hierarchical Task Networks agent implementation.

          Args:
              state_size: Dimension of state space
              action_size: Number of possible actions
              learning_rate: Learning rate for policies (default: 0.001)
              discount_factor: Discount factor gamma (default: 0.99)
          """

          def __init__(self, state_size: int, action_size: int,
                       learning_rate: float = 0.001, discount_factor: float = 0.99):

              self.state_size = state_size
              self.action_size = action_size
              self.alpha = learning_rate
              self.gamma = discount_factor

              # Task hierarchy
              self.task_hierarchy = self.build_task_hierarchy()

              # Policies for each task
              self.task_policies = {}

              # Experience buffers for each task
              self.task_buffers = {}

          def build_task_hierarchy(self) -> Task:
              """Build the hierarchical task structure."""
              # Example: Navigation task with subgoals
              navigate = Task("navigate", [
                  Task("move_to_waypoint", []),
                  Task("avoid_obstacles", []),
                  Task("reach_destination", [])
              ])

              return navigate

          def get_action(self, state: np.ndarray, current_task: Task) -> int:
              """Get action for current task."""
              if current_task.is_primitive():
                  # Execute primitive action
                  return self.get_primitive_action(state, current_task)
              else:
                  # Select subtask to execute
                  subtask = self.select_subtask(state, current_task)
                  return self.get_action(state, subtask)

          def get_primitive_action(self, state: np.ndarray, task: Task) -> int:
              """Get primitive action for a task."""
              if task.name not in self.task_policies:
                  # Initialize random policy
                  self.task_policies[task.name] = np.random.rand(self.state_size, self.action_size)
                  self.task_policies[task.name] = self.task_policies[task.name] / self.task_policies[task.name].sum(axis=1, keepdims=True)

              # Get action probabilities
              state_idx = self.state_to_index(state)
              action_probs = self.task_policies[task.name][state_idx]
              
              # Sample action
              return np.random.choice(self.action_size, p=action_probs)

          def select_subtask(self, state: np.ndarray, task: Task) -> Task:
              """Select which subtask to execute next."""
              if task.name not in self.task_policies:
                  # Initialize random policy
                  self.task_policies[task.name] = np.random.rand(self.state_size, len(task.subtasks))
                  self.task_policies[task.name] = self.task_policies[task.name] / self.task_policies[task.name].sum(axis=1, keepdims=True)

              # Get subtask probabilities
              state_idx = self.state_to_index(state)
              subtask_probs = self.task_policies[task.name][state_idx]
              
              # Sample subtask
              subtask_idx = np.random.choice(len(task.subtasks), p=subtask_probs)
              return task.subtasks[subtask_idx]

          def step(self, state: np.ndarray, action: int, reward: float,
                  next_state: np.ndarray, done: bool, current_task: Task) -> None:
              """Process one step and update task policies."""
              # Store experience
              if current_task.name not in self.task_buffers:
                  self.task_buffers[current_task.name] = []
              
              self.task_buffers[current_task.name].append((state, action, reward, next_state, done))

              # Update policy if enough experience
              if len(self.task_buffers[current_task.name]) >= 10:
                  self.update_task_policy(current_task)

          def update_task_policy(self, task: Task) -> None:
              """Update policy for a specific task."""
              if task.name not in self.task_buffers or len(self.task_buffers[task.name]) < 10:
                  return

              # Sample batch from buffer
              buffer = self.task_buffers[task.name]
              batch = np.random.choice(len(buffer), min(32, len(buffer)), replace=False)

              states, actions, rewards, next_states, dones = zip(*[buffer[i] for i in batch])

              # Convert to indices
              state_indices = [self.state_to_index(s) for s in states]
              next_state_indices = [self.state_to_index(s) for s in next_states]

              # Update Q-values
              for i, (s_idx, a, r, s_next_idx, done) in enumerate(zip(state_indices, actions, rewards, next_state_indices, dones)):
                  if task.is_primitive():
                      # Update primitive task policy
                      current_q = self.task_policies[task.name][s_idx, a]
                      if done:
                          target = r
                      else:
                          target = r + self.gamma * np.max(self.task_policies[task.name][s_next_idx])
                      
                      self.task_policies[task.name][s_idx, a] = current_q + self.alpha * (target - current_q)
                  else:
                      # Update subtask selection policy
                      current_q = self.task_policies[task.name][s_idx, a]
                      if done:
                          target = r
                      else:
                          target = r + self.gamma * np.max(self.task_policies[task.name][s_next_idx])
                      
                      self.task_policies[task.name][s_idx, a] = current_q + self.alpha * (target - current_q)

          def state_to_index(self, state: np.ndarray) -> int:
              """Convert continuous state to discrete index."""
              # Simple discretization - in practice, this would be more sophisticated
              return int(np.clip(np.sum(state), 0, self.state_size - 1))

          def get_task_value(self, task: Task, state: np.ndarray) -> float:
              """Get value of a task in a given state."""
              if task.is_primitive():
                  state_idx = self.state_to_index(state)
                  return np.max(self.task_policies[task.name][state_idx])
              else:
                  # Value is maximum over subtasks
                  return max(self.get_task_value(subtask, state) for subtask in task.subtasks)

          def plan_execution(self, state: np.ndarray, root_task: Task) -> List[Task]:
              """Plan execution sequence for a task."""
              if root_task.is_primitive():
                  return [root_task]
              else:
                  # Select best subtask
                  best_subtask = self.select_subtask(state, root_task)
                  return [root_task] + self.plan_execution(state, best_subtask)
    advantages:
      - "Natural task decomposition for complex problems"
      - "Modular learning allows independent subtask training"
      - "Temporal abstraction enables planning at different levels"
      - "Reusable subtasks can be applied to different composite tasks"
    disadvantages:
      - "Requires manual design of task hierarchy"
      - "Task decomposition can be challenging"
      - "Coordination between subtasks can be complex"
      - "May not discover optimal task decompositions automatically"

# Complexity analysis
complexity:
  analysis:
    - approach: "Basic HTN"
      time: "O(|T| × |S| × |A| × episodes)"
      space: "O(|T| × |S| × |A|)"
      notes: "Time complexity depends on number of tasks, state space, and episodes. Space grows with task hierarchy size"
    
    - approach: "HTN with Planning"
      time: "O(|T| × |S| × |A| × episodes + planning_time)"
      space: "O(|T| × |S| × |A| + planning_space)"
      notes: "Planning adds overhead but can improve task execution efficiency"

# Applications and use cases
applications:
  - category: "Robotics and Control"
    examples:
      - "Robot Manipulation: Complex manipulation tasks with hierarchical subtasks"
      - "Autonomous Navigation: Multi-level navigation with waypoint and obstacle avoidance subtasks"
      - "Industrial Automation: Process control with hierarchical task decomposition"
      - "Swarm Robotics: Coordinated behavior with hierarchical task networks"

  - category: "Game AI and Strategy"
    examples:
      - "Strategy Games: Multi-level decision making with hierarchical task planning"
      - "Puzzle Games: Complex puzzles with hierarchical solution strategies"
      - "Adventure Games: Quest completion with hierarchical task networks"
      - "Simulation Games: Resource management with hierarchical task planning"

  - category: "Real-World Applications"
    examples:
      - "Autonomous Vehicles: Multi-level driving with hierarchical task decomposition"
      - "Healthcare: Treatment planning with hierarchical medical task networks"
      - "Finance: Portfolio management with hierarchical investment task networks"
      - "Network Control: Traffic management with hierarchical routing task networks"

  - category: "Educational Value"
    examples:
      - "Task Decomposition: Understanding how to break complex problems into simpler parts"
      - "Hierarchical Planning: Learning to plan at multiple levels of abstraction"
      - "Modular Learning: Understanding how to learn components independently"
      - "Transfer Learning: Learning reusable skills across different composite tasks"

# Educational value and learning objectives
educational_value:
  - "Task Decomposition: Perfect introduction to breaking complex problems into simpler parts"
  - "Hierarchical Planning: Shows how to plan at multiple levels of abstraction"
  - "Modular Learning: Demonstrates learning components independently"
  - "Transfer Learning: Illustrates how skills can be reused across different tasks"

# Implementation status and development info
status:
  current: "planned"
  implementation_quality: "none"
  test_coverage: "none"
  documentation_quality: "planned"

  # Source code locations
  source_files:
    - path: "src/algokit/hierarchical_rl/hierarchical_task_networks.py"
      description: "Main implementation with task hierarchy and subtask policies"
    - path: "tests/unit/hierarchical_rl/test_hierarchical_task_networks.py"
      description: "Comprehensive test suite including task decomposition tests"

# References and resources - structured format for template rendering
references:
  - category: "Core Papers"
    items:
      - bib_key: "andreas2017"
        note: "Modular multitask reinforcement learning with policy sketches"
      - bib_key: "kaelbling1998"
        note: "Foundational work on hierarchical reinforcement learning"

  - category: "Hierarchical RL Textbooks"
    items:
      - bib_key: "sutton2018"
        note: "Comprehensive introduction to reinforcement learning including hierarchical methods"
      - bib_key: "szepesvari2010"
        note: "Algorithms for reinforcement learning with hierarchical approaches"

  - category: "Online Resources"
    items:
      - title: "Hierarchical Task Networks"
        url: "https://en.wikipedia.org/wiki/Hierarchical_task_network"
        note: "Wikipedia article on hierarchical task networks"
      - title: "Task Decomposition in AI"
        url: "https://www.cs.cmu.edu/~mmv/papers/kaelbling-aaai99.pdf"
        note: "Task decomposition and hierarchical planning in AI"

  - category: "Implementation & Practice"
    items:
      - title: "OpenAI Gym"
        url: "https://www.gymlibrary.dev/"
        note: "RL environments for testing hierarchical task networks"
      - title: "Stable Baselines3"
        url: "https://stable-baselines3.readthedocs.io/"
        note: "High-quality RL algorithm implementations"
      - title: "Ray RLlib"
        url: "https://docs.ray.io/en/latest/rllib/"
        note: "Scalable RL library for production use"

# Tags for categorization and search
tags:
  - "hierarchical-rl"
  - "hierarchical-task-networks"
  - "htn"
  - "task-decomposition"
  - "planning"
  - "temporal-abstraction"
  - "algorithms"

# Related algorithms and cross-references
related_algorithms:
  - slug: "feudal-networks"
    relationship: "same_family"
    description: "Another hierarchical RL approach with manager-worker architecture"
  - slug: "option-critic"
    relationship: "same_family"
    description: "Option-based hierarchical RL with automatic option discovery"
