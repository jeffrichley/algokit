# Enhanced Algorithm Schema for Algorithm Documentation
# This schema supports all algorithm types with rich metadata and structured content

# Basic metadata
slug: hierarchical-actor-critic
name: Hierarchical Actor-Critic (HAC)
family_id: hierarchical-rl

# Brief one-sentence summary for cards and navigation
hidden: true  # Hidden by default
summary: "An advanced hierarchical reinforcement learning algorithm that extends the actor-critic framework with temporal abstraction and hierarchical structure."

# Detailed description (markdown supported) - full overview for the algorithm page
description: |
  Hierarchical Actor-Critic (HAC) is an advanced reinforcement learning algorithm that extends the
  actor-critic framework with temporal abstraction and hierarchical structure. The algorithm operates
  at multiple levels: a high-level meta-policy that selects subgoals or options, and low-level
  policies that execute actions to achieve these subgoals.

  This hierarchical approach enables the agent to solve complex, long-horizon tasks by breaking them
  down into manageable subproblems. The meta-policy learns to sequence subgoals effectively, while
  the low-level policies learn to achieve specific subgoals efficiently. HAC is particularly powerful
  in domains where tasks have natural hierarchical structure, such as robotics manipulation, navigation,
  and game playing.

# Problem formulation and mathematical details
formulation:
  problem_definition: |
    Given:
    - State space: S
    - Subgoal space: G
    - Action space: A
    - Meta-policy: π_meta(g_t|s_t)
    - Low-level policy: π_low(a_t|s_t, g_t)
    - Meta-critic: V_meta(s_t)
    - Low-level critic: V_low(s_t, g_t)
    - Reward function: R(s,a,s')

    Find hierarchical actor-critic policies that maximize expected cumulative reward:

    π_h(a_t|s_t) = ∑_{g_t} π_meta(g_t|s_t) · π_low(a_t|s_t, g_t)

  key_properties:
    - name: "Hierarchical Policy Decomposition"
      formula: "π_h(a_t|s_t) = ∑_{g_t} π_meta(g_t|s_t) · π_low(a_t|s_t, g_t)"
      description: "Hierarchical policy decomposes into meta and low-level components"
    - name: "Hierarchical Value Function"
      formula: "V_h(s_t) = E_{g_t ~ π_meta}[V_low(s_t, g_t)]"
      description: "Value function decomposes into meta and low-level components"
    - name: "Hierarchical Advantage Function"
      formula: "A_h(s_t, g_t, a_t) = Q_h(s_t, g_t, a_t) - V_h(s_t)"
      description: "Advantage function for hierarchical policy updates"

# Key properties and characteristics
properties:
  - name: "Temporal Abstraction"
    description: "High-level policies operate over longer time horizons"
    importance: "fundamental"
  - name: "Subgoal Decomposition"
    description: "Complex tasks broken into manageable subproblems"
    importance: "fundamental"
  - name: "Hierarchical Learning"
    description: "Policies and critics at different levels learn simultaneously"
    importance: "fundamental"
  - name: "Transfer Learning"
    description: "Low-level policies can be reused across different tasks"
    importance: "implementation"

# Implementation approaches with detailed code
implementations:
  - type: "basic_hierarchical_actor_critic"
    name: "Basic Hierarchical Actor-Critic (Recommended)"
    description: "Standard HAC implementation with meta and low-level actor-critic networks"
    complexity:
      time: "O(batch_size × (meta_actor_params + meta_critic_params + low_actor_params + low_critic_params))"
      space: "O(batch_size × (state_size + subgoal_size))"
    code: |
      import torch
      import torch.nn as nn
      import torch.optim as optim
      import torch.nn.functional as F
      import numpy as np

      class MetaActorNetwork(nn.Module):
          """Meta-actor network that selects subgoals."""

          def __init__(self, state_size: int, subgoal_size: int, hidden_size: int = 128):
              super(MetaActorNetwork, self).__init__()
              self.fc1 = nn.Linear(state_size, hidden_size)
              self.fc2 = nn.Linear(hidden_size, hidden_size)
              self.fc3 = nn.Linear(hidden_size, subgoal_size)

          def forward(self, state: torch.Tensor) -> torch.Tensor:
              x = F.relu(self.fc1(state))
              x = F.relu(self.fc2(x))
              subgoal_probs = F.softmax(self.fc3(x), dim=-1)
              return subgoal_probs

      class MetaCriticNetwork(nn.Module):
          """Meta-critic network that estimates state values."""

          def __init__(self, state_size: int, hidden_size: int = 128):
              super(MetaCriticNetwork, self).__init__()
              self.fc1 = nn.Linear(state_size, hidden_size)
              self.fc2 = nn.Linear(hidden_size, hidden_size)
              self.fc3 = nn.Linear(hidden_size, 1)

          def forward(self, state: torch.Tensor) -> torch.Tensor:
              x = F.relu(self.fc1(state))
              x = F.relu(self.fc2(x))
              value = self.fc3(x)
              return value

      class LowLevelActorNetwork(nn.Module):
          """Low-level actor network that executes actions given subgoals."""

          def __init__(self, state_size: int, subgoal_size: int, action_size: int, hidden_size: int = 128):
              super(LowLevelActorNetwork, self).__init__()
              self.fc1 = nn.Linear(state_size + subgoal_size, hidden_size)
              self.fc2 = nn.Linear(hidden_size, hidden_size)
              self.fc3 = nn.Linear(hidden_size, action_size)

          def forward(self, state: torch.Tensor, subgoal: torch.Tensor) -> torch.Tensor:
              # Concatenate state and subgoal
              combined = torch.cat([state, subgoal], dim=-1)
              x = F.relu(self.fc1(combined))
              x = F.relu(self.fc2(x))
              action_probs = F.softmax(self.fc3(x), dim=-1)
              return action_probs

      class LowLevelCriticNetwork(nn.Module):
          """Low-level critic network that estimates state-subgoal values."""

          def __init__(self, state_size: int, subgoal_size: int, hidden_size: int = 128):
              super(LowLevelCriticNetwork, self).__init__()
              self.fc1 = nn.Linear(state_size + subgoal_size, hidden_size)
              self.fc2 = nn.Linear(hidden_size, hidden_size)
              self.fc3 = nn.Linear(hidden_size, 1)

          def forward(self, state: torch.Tensor, subgoal: torch.Tensor) -> torch.Tensor:
              # Concatenate state and subgoal
              combined = torch.cat([state, subgoal], dim=-1)
              x = F.relu(self.fc1(combined))
              x = F.relu(self.fc2(x))
              value = self.fc3(x)
              return value

      class HierarchicalActorCriticAgent:
          """
          Hierarchical Actor-Critic agent implementation.

          Args:
              state_size: Dimension of state space
              subgoal_size: Dimension of subgoal space
              action_size: Number of possible actions
              meta_actor_lr: Learning rate for meta-actor (default: 0.001)
              meta_critic_lr: Learning rate for meta-critic (default: 0.001)
              low_actor_lr: Learning rate for low-level actor (default: 0.001)
              low_critic_lr: Learning rate for low-level critic (default: 0.001)
              discount_factor: Discount factor gamma (default: 0.99)
              subgoal_horizon: Maximum steps to achieve subgoal (default: 50)
          """

          def __init__(self, state_size: int, subgoal_size: int, action_size: int,
                       meta_actor_lr: float = 0.001, meta_critic_lr: float = 0.001,
                       low_actor_lr: float = 0.001, low_critic_lr: float = 0.001,
                       discount_factor: float = 0.99, subgoal_horizon: int = 50):

              self.state_size = state_size
              self.subgoal_size = subgoal_size
              self.action_size = action_size
              self.gamma = discount_factor
              self.subgoal_horizon = subgoal_horizon

              # Networks
              self.meta_actor = MetaActorNetwork(state_size, subgoal_size)
              self.meta_critic = MetaCriticNetwork(state_size)
              self.low_actor = LowLevelActorNetwork(state_size, subgoal_size, action_size)
              self.low_critic = LowLevelCriticNetwork(state_size, subgoal_size)

              # Optimizers
              self.meta_actor_optimizer = optim.Adam(self.meta_actor.parameters(), lr=meta_actor_lr)
              self.meta_critic_optimizer = optim.Adam(self.meta_critic.parameters(), lr=meta_critic_lr)
              self.low_actor_optimizer = optim.Adam(self.low_actor.parameters(), lr=low_actor_lr)
              self.low_critic_optimizer = optim.Adam(self.low_critic.parameters(), lr=low_critic_lr)

              # Experience buffers
              self.meta_buffer = []
              self.low_buffer = []

              # Current subgoal tracking
              self.current_subgoal = None
              self.subgoal_steps = 0

          def get_subgoal(self, state: np.ndarray) -> tuple[int, float]:
              """Select subgoal using meta-actor network."""
              state_tensor = torch.FloatTensor(state).unsqueeze(0)
              subgoal_probs = self.meta_actor(state_tensor)

              # Sample subgoal
              dist = torch.distributions.Categorical(subgoal_probs)
              subgoal = dist.sample()
              log_prob = dist.log_prob(subgoal)

              return subgoal.item(), log_prob.item()

          def get_action(self, state: np.ndarray, subgoal: int) -> tuple[int, float]:
              """Get action using low-level actor network given subgoal."""
              state_tensor = torch.FloatTensor(state).unsqueeze(0)
              subgoal_tensor = torch.FloatTensor([subgoal]).unsqueeze(0)

              action_probs = self.low_actor(state_tensor, subgoal_tensor)

              # Sample action
              dist = torch.distributions.Categorical(action_probs)
              action = dist.sample()
              log_prob = dist.log_prob(action)

              return action.item(), log_prob.item()

          def step(self, state: np.ndarray, action: int, reward: float,
                  next_state: np.ndarray, done: bool) -> None:
              """Process one step and potentially update subgoal."""
              # Store low-level transition
              if self.current_subgoal is not None:
                  self.low_buffer.append((state, self.current_subgoal, action, reward, next_state, done))

              # Check if subgoal should be updated
              self.subgoal_steps += 1
              if (self.subgoal_steps >= self.subgoal_horizon or
                  self.is_subgoal_achieved(state, next_state) or done):

                  # Store meta transition
                  if len(self.low_buffer) > 0:
                      total_reward = sum(r for _, _, _, r, _, _ in self.low_buffer)
                      self.meta_buffer.append((state, self.current_subgoal, total_reward, next_state, done))

                  # Select new subgoal
                  if not done:
                      new_subgoal, _ = self.get_subgoal(next_state)
                      self.current_subgoal = new_subgoal
                      self.subgoal_steps = 0

                  # Clear low-level buffer
                  self.low_buffer.clear()

          def is_subgoal_achieved(self, state: np.ndarray, next_state: np.ndarray) -> bool:
              """Check if current subgoal has been achieved."""
              # Simple distance-based subgoal achievement
              # In practice, this would be domain-specific
              return np.linalg.norm(next_state - state) < 0.1

          def update_networks(self) -> None:
              """Update all actor-critic networks."""
              self.update_low_actor_critic()
              self.update_meta_actor_critic()

          def update_low_actor_critic(self) -> None:
              """Update low-level actor-critic networks."""
              if len(self.low_buffer) < 10:
                  return

              # Sample batch from low-level buffer
              batch = np.random.choice(len(self.low_buffer), min(32, len(self.low_buffer)), replace=False)

              states, subgoals, actions, rewards, next_states, dones = zip(*[self.low_buffer[i] for i in batch])

              # Convert to tensors
              states = torch.FloatTensor(states)
              subgoals = torch.LongTensor(subgoals)
              actions = torch.LongTensor(actions)
              rewards = torch.FloatTensor(rewards)
              next_states = torch.FloatTensor(next_states)
              dones = torch.BoolTensor(dones)

              # Update low-level critic
              current_values = self.low_critic(states, F.one_hot(subgoals, self.subgoal_size).float()).squeeze()
              next_values = self.low_critic(next_states, F.one_hot(subgoals, self.subgoal_size).float()).squeeze()
              
              target_values = rewards + self.gamma * next_values * ~dones
              critic_loss = F.mse_loss(current_values, target_values.detach())

              self.low_critic_optimizer.zero_grad()
              critic_loss.backward()
              self.low_critic_optimizer.step()

              # Update low-level actor
              action_probs = self.low_actor(states, F.one_hot(subgoals, self.subgoal_size).float())
              dist = torch.distributions.Categorical(action_probs)
              log_probs = dist.log_prob(actions)

              advantages = target_values - current_values.detach()
              actor_loss = -(log_probs * advantages).mean()

              self.low_actor_optimizer.zero_grad()
              actor_loss.backward()
              self.low_actor_optimizer.step()

          def update_meta_actor_critic(self) -> None:
              """Update meta-level actor-critic networks."""
              if len(self.meta_buffer) < 10:
                  return

              # Sample batch from meta buffer
              batch = np.random.choice(len(self.meta_buffer), min(32, len(self.meta_buffer)), replace=False)

              states, subgoals, rewards, next_states, dones = zip(*[self.meta_buffer[i] for i in batch])

              # Convert to tensors
              states = torch.FloatTensor(states)
              subgoals = torch.LongTensor(subgoals)
              rewards = torch.FloatTensor(rewards)
              next_states = torch.FloatTensor(next_states)
              dones = torch.BoolTensor(dones)

              # Update meta-critic
              current_values = self.meta_critic(states).squeeze()
              next_values = self.meta_critic(next_states).squeeze()
              
              target_values = rewards + self.gamma * next_values * ~dones
              critic_loss = F.mse_loss(current_values, target_values.detach())

              self.meta_critic_optimizer.zero_grad()
              critic_loss.backward()
              self.meta_critic_optimizer.step()

              # Update meta-actor
              subgoal_probs = self.meta_actor(states)
              dist = torch.distributions.Categorical(subgoal_probs)
              log_probs = dist.log_prob(subgoals)

              advantages = target_values - current_values.detach()
              actor_loss = -(log_probs * advantages).mean()

              self.meta_actor_optimizer.zero_grad()
              actor_loss.backward()
              self.meta_actor_optimizer.step()
    advantages:
      - "Combines benefits of actor-critic methods with hierarchical structure"
      - "Temporal abstraction enables learning at different time scales"
      - "Subgoal decomposition makes complex tasks manageable"
      - "Value functions reduce variance compared to pure policy gradient methods"
    disadvantages:
      - "Requires careful coordination between multiple networks"
      - "Subgoal achievement detection can be challenging"
      - "Four networks increase complexity and training time"
      - "Hyperparameter tuning becomes more complex"

# Complexity analysis
complexity:
  analysis:
    - approach: "Basic HAC"
      time: "O(batch_size × (meta_actor_params + meta_critic_params + low_actor_params + low_critic_params))"
      space: "O(batch_size × (state_size + subgoal_size))"
      notes: "Four-network architecture requires careful coordination and training"
    
    - approach: "HAC with Experience Replay"
      time: "O(batch_size × (meta_actor_params + meta_critic_params + low_actor_params + low_critic_params))"
      space: "O(batch_size × (state_size + subgoal_size) + buffer_size)"
      notes: "Experience replay improves sample efficiency but increases memory usage"

# Applications and use cases
applications:
  - category: "Robotics and Control"
    examples:
      - "Robot Manipulation: Complex manipulation tasks with hierarchical subgoals"
      - "Autonomous Navigation: Multi-level navigation with waypoint subgoals"
      - "Industrial Automation: Process control with hierarchical objectives"
      - "Swarm Robotics: Coordinated behavior with hierarchical task decomposition"

  - category: "Game AI and Strategy"
    examples:
      - "Strategy Games: Multi-level decision making with tactical and strategic goals"
      - "Puzzle Games: Complex puzzles broken into simpler subproblems"
      - "Adventure Games: Quest completion with hierarchical objectives"
      - "Simulation Games: Resource management with hierarchical planning"

  - category: "Real-World Applications"
    examples:
      - "Autonomous Vehicles: Multi-level driving with navigation and control subgoals"
      - "Healthcare: Treatment planning with hierarchical medical objectives"
      - "Finance: Portfolio management with hierarchical investment strategies"
      - "Network Control: Traffic management with hierarchical routing policies"

  - category: "Educational Value"
    examples:
      - "Hierarchical Learning: Understanding multi-level decision making"
      - "Actor-Critic Methods: Learning value functions and policies simultaneously"
      - "Temporal Abstraction: Understanding different time scales in learning"
      - "Transfer Learning: Learning reusable skills across different tasks"

# Educational value and learning objectives
educational_value:
  - "Hierarchical Learning: Perfect introduction to multi-level decision making"
  - "Actor-Critic Methods: Shows how to combine value functions and policies"
  - "Temporal Abstraction: Demonstrates learning at different time scales"
  - "Transfer Learning: Illustrates how skills can be reused across tasks"

# Implementation status and development info
status:
  current: "planned"
  implementation_quality: "none"
  test_coverage: "none"
  documentation_quality: "planned"

  # Source code locations
  source_files:
    - path: "src/algokit/hierarchical_rl/hierarchical_actor_critic.py"
      description: "Main implementation with meta and low-level actor-critic networks"
    - path: "tests/unit/hierarchical_rl/test_hierarchical_actor_critic.py"
      description: "Comprehensive test suite including convergence tests"

# References and resources - structured format for template rendering
references:
  - category: "Core Papers"
    items:
      - bib_key: "levy2017"
        note: "Original Hierarchical Actor-Critic paper introducing HAC"
      - bib_key: "kaelbling1998"
        note: "Foundational work on hierarchical reinforcement learning"

  - category: "Hierarchical RL Textbooks"
    items:
      - bib_key: "sutton2018"
        note: "Comprehensive introduction to reinforcement learning including hierarchical methods"
      - bib_key: "szepesvari2010"
        note: "Algorithms for reinforcement learning with hierarchical approaches"

  - category: "Online Resources"
    items:
      - title: "Hierarchical Actor-Critic"
        url: "https://github.com/andrew-j-levy/Hierarchical-Actor-Critic-HAC-"
        note: "Official HAC implementation repository"
      - title: "Hierarchical Reinforcement Learning"
        url: "https://en.wikipedia.org/wiki/Hierarchical_reinforcement_learning"
        note: "Wikipedia article on hierarchical reinforcement learning"

  - category: "Implementation & Practice"
    items:
      - title: "PyTorch Documentation"
        url: "https://pytorch.org/docs/"
        note: "PyTorch deep learning framework documentation"
      - title: "OpenAI Gym"
        url: "https://www.gymlibrary.dev/"
        note: "RL environments for testing hierarchical algorithms"
      - title: "Stable Baselines3"
        url: "https://stable-baselines3.readthedocs.io/"
        note: "High-quality RL algorithm implementations"

# Tags for categorization and search
tags:
  - "hierarchical-rl"
  - "hierarchical-actor-critic"
  - "hac"
  - "temporal-abstraction"
  - "subgoal-decomposition"
  - "actor-critic"
  - "algorithms"

# Related algorithms and cross-references
related_algorithms:
  - slug: "actor-critic"
    relationship: "parent"
    description: "Traditional actor-critic that hierarchical actor-critic extends"
  - slug: "hierarchical-policy-gradient"
    relationship: "same_family"
    description: "Policy gradient version of hierarchical learning"
