# ğŸ“˜ Roadmap: AI, Planning, and Control Algorithms

This document outlines a comprehensive roadmap for implementing and teaching a variety of foundational and advanced algorithms in AI, control, planning, and machine learning. It is designed to support instructional use, demos for students, and personal mastery. This project is named **AlgoKit**.

---

## ğŸ” Step-by-Step Plan for AlgoKit

### Phase 1: Setup and Scaffolding

1. âœ… Initialize GitHub repository: `algokit`
2. âœ… Create top-level folder structure (see end of doc)
3. âœ… Create template README and CONTRIBUTING docs
4. âœ… Define testing framework (e.g. `pytest`, `unittest`)
5. âœ… Set up CI (GitHub Actions or similar)

### Phase 2: Core Implementations

6. ğŸ§  Implement Classic DP problems (week-by-week)
7. ğŸ§ª Implement core RL algorithms (Q, SARSA, PPO, etc.)
8. ğŸ§± Build HRL scaffolding (options, subgoal modeling)
9. ğŸ§¬ Add DMP encoding/decoding + demo tasks
10. ğŸ“ˆ Integrate GP regression and Bayesian opt
11. â± Build PID + Kalman controller demos
12. ğŸ“‰ Implement MPC with linear and nonlinear examples
13. ğŸ§­ Add optional planning tools (A\*, RRT, etc.)

### Phase 3: Visualization and Teaching Support

14. ğŸ“ Write Jupyter notebooks for each algorithm
15. ğŸ“Š Add animated visualizations (matplotlib, gifs)
16. ğŸ§‘â€ğŸ« Create sample lecture slides (reveal.js or pptx)
17. ğŸ“ Add student exercises and challenge problems

### Phase 4: Final Polish and Deployment

18. ğŸ§¹ Clean and lint all code (ruff, black, mypy)
19. ğŸš€ Publish as a JupyterBook and/or GitHub Pages
20. ğŸ” Add plug-and-play `main.py` launcher for CLI use

---

## 1. Classic Dynamic Programming (DP)

### Goal:

Model problems with optimal substructure and overlapping subproblems.

### Categories & Algorithms:

#### 1.1 Linear DP

* Fibonacci (memoized/tabulated)
* Longest Increasing Subsequence (LIS)
* Maximum Subarray (Kadaneâ€™s Algorithm)
* House Robber
* Climbing Stairs
* Jump Game I & II

#### 1.2 2D / Grid-Based DP

* Unique Paths
* Minimum Path Sum
* Longest Common Subsequence (LCS)
* Edit Distance
* Maximum Square Submatrix

#### 1.3 Knapsack Variants

* 0/1 Knapsack
* Unbounded Knapsack
* Partition Equal Subset Sum
* Target Sum

#### 1.4 String DP

* LCS
* Edit Distance
* Regular Expression Matching
* Wildcard Matching
* Palindromic Substrings

#### 1.5 Interval DP

* Matrix Chain Multiplication
* Burst Balloons
* Palindrome Partitioning

#### 1.6 Bitmask / Digit DP

* Traveling Salesman Problem (TSP)
* Assignment Problem
* Digit Sum Count

#### 1.7 Tree/DAG DP

* Diameter of Tree
* House Robber III
* Longest Path in DAG

---

## 2. Reinforcement Learning (Model-Free)

### Goal:

Teach agents to learn optimal behavior through interaction with an environment.

### Algorithms:

* Q-Learning
* SARSA
* Deep Q-Network (DQN)
* Proximal Policy Optimization (PPO)
* A2C / A3C

### Concepts:

* Exploration vs exploitation
* Discounted return
* Temporal difference learning
* Experience replay and target networks

---

## 3. Hierarchical Reinforcement Learning (HRL)

### Goal:

Introduce temporal abstraction and multi-level decision-making.

### Algorithms:

* Options Framework
* Feudal Reinforcement Learning
* MAXQ Decomposition
* HIRO (state-of-the-art)

### Concepts:

* Subgoals
* Macro-actions
* Inter-option policy training

---

## 4. Dynamic Movement Primitives (DMPs)

### Goal:

Encode reusable trajectories for motion planning.

### Algorithms:

* DMP encoding/decoding
* Imitation learning using DMPs

### Concepts:

* Nonlinear dynamical systems
* Attractor landscapes
* Temporal scaling

---

## 5. Gaussian Process Modeling

### Goal:

Model functions with uncertainty using Bayesian regression.

### Algorithms:

* GP Regression
* Sparse GPs
* Bayesian Optimization
* PILCO (Model-based RL)

### Concepts:

* Kernel functions
* Posterior predictive distribution
* Acquisition functions (UCB, EI)

---

## 6. Real-Time Control

### Goal:

Implement fast, reactive control systems with feedback.

### Algorithms:

* PID Controller (P/PI/PD/PID)
* Bang-bang control
* Kalman Filter

### Concepts:

* Feedback vs feedforward
* Tuning gains
* Real-time latency constraints

---

## 7. Model Predictive Control (MPC)

### Goal:

Use optimization to plan control actions over a horizon.

### Algorithms:

* Finite Horizon MPC
* Nonlinear MPC
* Learning-based MPC

### Concepts:

* Cost functions
* State and control constraints
* Receding horizon optimization

---

## 8. (Optional) Classical Planning Algorithms

### Goal:

Solve planning problems in state or configuration space.

### Algorithms:

* A\*
* Dijkstraâ€™s
* RRT / PRM (sampling-based)
* STRIPS / PDDL (symbolic AI)

### Concepts:

* Heuristics
* Search graph expansion
* Plan execution and validation

---

## Suggested Structure

```bash
algokit/
â”œâ”€â”€ classic_dp/
â”œâ”€â”€ reinforcement_learning/
â”œâ”€â”€ hrl/
â”œâ”€â”€ dmps/
â”œâ”€â”€ gaussian_process/
â”œâ”€â”€ real_time_control/
â”œâ”€â”€ mpc/
â”œâ”€â”€ classical_planning/
â”œâ”€â”€ notebooks/
â””â”€â”€ README.md
```

Each folder should include:

* Standalone implementations
* Visualization and test cases
* Notebook versions for demo/teaching
* Annotated examples with didactic comments

---

Ready to start building out each module or publishing this as a GitHub repo or JupyterBook.
